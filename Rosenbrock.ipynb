{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n#  TP Optimisation II - Fonction de Rosenbrock\n## **INF4127 : Optimisation numérique sans contraintes** ##\n\n## **Université de Yaoundé 1 - Département d'Informatique** ##\n \n### **Année académique 2025-2026** ###\n\n\n# ---\n#\n\n#### **Objectif** \n#### Reproduire les algorithmes de descente de gradient sur la fonction de Rosenbrock : \n \n### $$f(x, y) = (1 - x)^2 + 100(y - x^2)^2$$ ###\n\n #### Nous comparerons deux méthodes de descente de gradient : #####\n #### 1. **Gradient à pas optimal** (méthode de plus profonde descente) #####\n#### 2. **Gradient à pas fixe** (avec différentes valeurs du pas) #####","metadata":{"editable":false}},{"cell_type":"markdown","source":"\n#### **1. Imports et Configuration** ","metadata":{"editable":false}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration pour de meilleurs graphiques\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.grid'] = True\n\nprint(\"Bibliothèques chargées avec succès !\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **2. Définition de la fonction de Rosenbrock** ####\n\n#### La fonction de Rosenbrock, aussi appelée fonction \"banane\" ou \"vallée\", est un cas test classique en optimisation.\n\n#### **Propriétés :**\n#### - Minimum global unique en $(x^*, y^*) = (1, 1)$ avec $f(1, 1) = 0$\n#### - Fonction non-convexe\n#### - Vallée étroite et incurvée menant au minimum\n#### - Difficile à optimiser avec des méthodes simples","metadata":{"editable":false}},{"cell_type":"code","source":"def rosenbrock(x, y):\n    \"\"\"\n    Calcule la valeur de la fonction de Rosenbrock.\n    f(x, y) = (1 - x)² + 100(y - x²)²\n    \n    Le minimum global est en (1, 1) avec f(1, 1) = 0\n    \"\"\"\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\n\ndef gradient_rosenbrock(x, y):\n    \"\"\"\n    Calcule le gradient de la fonction de Rosenbrock.\n    \n    ∇f = (∂f/∂x, ∂f/∂y)\n    ∂f/∂x = -2(1 - x) - 400x(y - x²)\n    ∂f/∂y = 200(y - x²)\n    \n    Retourne un tuple (df_dx, df_dy)\n    \"\"\"\n    df_dx = -2 * (1 - x) - 400 * x * (y - x**2)\n    df_dy = 200 * (y - x**2)\n    return df_dx, df_dy\n\n\n# Test de la fonction\nx_test, y_test = 0, 0\nprint(f\"f({x_test}, {y_test}) = {rosenbrock(x_test, y_test)}\")\nprint(f\"∇f({x_test}, {y_test}) = {gradient_rosenbrock(x_test, y_test)}\")\nprint(f\"f(1, 1) = {rosenbrock(1, 1)} (minimum global)\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3. Algorithme de Gradient à Pas Optimal**\n\n#### Cette méthode, appelée **méthode de plus profonde descente**, calcule à chaque itération le pas optimal $s_k$ qui minimise la fonction dans la direction du gradient :\n\n#### $$s_k = \\arg\\min_{s>0} f(x_k - s\\nabla f(x_k))$$\n\n#### **Propriété importante** (vue en cours) : Deux directions de descente successives sont orthogonales, ce qui cause un comportement en zigzag.\n","metadata":{"editable":false}},{"cell_type":"code","source":"def recherche_lineaire_rosenbrock(x, y, dx, dy):\n    \"\"\"\n    Recherche linéaire approximative pour trouver le pas optimal.\n    \n    On cherche s qui minimise φ(s) = f(x - s*dx, y - s*dy)\n    où (dx, dy) est la direction de descente (le gradient).\n    \n    Méthode : échantillonnage sur un intervalle puis raffinement.\n    \"\"\"\n    # Recherche grossière sur un intervalle large\n    s_values = np.linspace(0.0001, 1, 100)\n    phi_values = [rosenbrock(x - s*dx, y - s*dy) for s in s_values]\n    \n    # Trouver le meilleur s dans l'échantillon grossier\n    idx_min = np.argmin(phi_values)\n    s_best = s_values[idx_min]\n    \n    # Raffinement autour du meilleur s trouvé\n    s_min = max(0.0001, s_best - 0.01)\n    s_max = s_best + 0.01\n    s_values_fine = np.linspace(s_min, s_max, 50)\n    phi_values_fine = [rosenbrock(x - s*dx, y - s*dy) for s in s_values_fine]\n    \n    idx_min_fine = np.argmin(phi_values_fine)\n    s_optimal = s_values_fine[idx_min_fine]\n    \n    return s_optimal\n\n\ndef gradient_pas_optimal(x0, y0, epsilon=1e-5, max_iter=10000):\n    \"\"\"\n    Méthode de plus profonde descente (steepest descent).\n    \n    À chaque itération, on cherche le pas optimal qui minimise\n    f(x_k - s*∇f(x_k)) par rapport à s.\n    \n    Paramètres:\n    -----------\n    x0, y0 : float\n        Point de départ\n    epsilon : float\n        Critère d'arrêt sur la norme du gradient\n    max_iter : int\n        Nombre maximum d'itérations\n    \n    Retourne:\n    ---------\n    trajectory : list of dict\n        Liste contenant l'historique des itérations\n    \"\"\"\n    # Initialisation\n    x, y = x0, y0\n    trajectory = []\n    \n    # Enregistrer le point initial\n    trajectory.append({\n        'iter': 0,\n        'x': x,\n        'y': y,\n        'f': rosenbrock(x, y),\n        'grad_norm': np.linalg.norm(gradient_rosenbrock(x, y)),\n        's_k': None\n    })\n    \n    for k in range(max_iter):\n        # Calculer le gradient au point courant\n        df_dx, df_dy = gradient_rosenbrock(x, y)\n        grad_norm = np.sqrt(df_dx**2 + df_dy**2)\n        \n        # Test de convergence : si le gradient est suffisamment petit, on s'arrête\n        if grad_norm < epsilon:\n            print(f\"Convergence atteinte à l'itération {k}\")\n            break\n        \n        # Recherche du pas optimal par recherche linéaire\n        s_optimal = recherche_lineaire_rosenbrock(x, y, df_dx, df_dy)\n        \n        # Mise à jour du point\n        x = x - s_optimal * df_dx\n        y = y - s_optimal * df_dy\n        \n        # Enregistrer les informations de cette itération\n        trajectory.append({\n            'iter': k + 1,\n            'x': x,\n            'y': y,\n            'f': rosenbrock(x, y),\n            'grad_norm': grad_norm,\n            's_k': s_optimal\n        })\n    \n    return trajectory\n\nprint(\"Algorithme de gradient à pas optimal défini !\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T15:50:01.645495Z","iopub.execute_input":"2025-11-23T15:50:01.646607Z","iopub.status.idle":"2025-11-23T15:50:01.677223Z","shell.execute_reply.started":"2025-11-23T15:50:01.64655Z","shell.execute_reply":"2025-11-23T15:50:01.670448Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **4. Algorithme de Gradient à Pas Fixe**\n\n#### Cette méthode utilise un pas constant $s$ à chaque itération :\n\n#### $$x_{k+1} = x_k - s\\nabla f(x_k)$$\n\n#### **Avantage** : Simple et peu coûteuse par itération\n\n#### **Inconvénient** : Le choix du pas est critique. Un pas trop grand fait diverger, un pas trop petit ralentit la convergence.\n","metadata":{"editable":false}},{"cell_type":"code","source":"def gradient_pas_fixe(x0, y0, s_fixe, epsilon=1e-5, max_iter=100000):\n    \"\"\"\n    Méthode de gradient à pas fixe.\n    \n    À chaque itération, on fait un pas de taille constante dans la direction\n    de la plus forte descente : x_{k+1} = x_k - s*∇f(x_k)\n    \n    Paramètres:\n    -----------\n    x0, y0 : float\n        Point de départ\n    s_fixe : float\n        Taille du pas (constante)\n    epsilon : float\n        Critère d'arrêt sur la norme du gradient\n    max_iter : int\n        Nombre maximum d'itérations\n    \n    Retourne:\n    ---------\n    trajectory : list of dict\n        Liste contenant l'historique des itérations\n    converged : bool\n        True si l'algorithme a convergé, False s'il a divergé\n    \"\"\"\n    # Initialisation\n    x, y = x0, y0\n    trajectory = []\n    converged = True\n    \n    # Enregistrer le point initial\n    trajectory.append({\n        'iter': 0,\n        'x': x,\n        'y': y,\n        'f': rosenbrock(x, y),\n        'grad_norm': np.linalg.norm(gradient_rosenbrock(x, y))\n    })\n    \n    for k in range(max_iter):\n        # Calculer le gradient au point courant\n        df_dx, df_dy = gradient_rosenbrock(x, y)\n        grad_norm = np.sqrt(df_dx**2 + df_dy**2)\n        \n        # Test de convergence\n        if grad_norm < epsilon:\n            print(f\"Convergence atteinte à l'itération {k}\")\n            break\n        \n        # Mise à jour avec pas fixe\n        x = x - s_fixe * df_dx\n        y = y - s_fixe * df_dy\n        \n        # Vérifier la divergence\n        if not np.isfinite(x) or not np.isfinite(y) or abs(x) > 1e10 or abs(y) > 1e10:\n            print(f\"Divergence détectée à l'itération {k}\")\n            converged = False\n            break\n        \n        # Enregistrer les informations de cette itération\n        trajectory.append({\n            'iter': k + 1,\n            'x': x,\n            'y': y,\n            'f': rosenbrock(x, y),\n            'grad_norm': grad_norm\n        })\n    \n    return trajectory, converged\n\nprint(\"Algorithme de gradient à pas fixe défini !\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:03:47.581552Z","iopub.execute_input":"2025-11-23T16:03:47.582086Z","iopub.status.idle":"2025-11-23T16:03:47.607894Z","shell.execute_reply.started":"2025-11-23T16:03:47.582053Z","shell.execute_reply":"2025-11-23T16:03:47.606779Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" #### **5. Fonctions de Visualisation**","metadata":{"editable":false}},{"cell_type":"code","source":"def plot_contours_with_trajectory(trajectory, title=\"Trajectoire d'optimisation\"):\n    \"\"\"\n    Visualise les courbes de niveau de la fonction de Rosenbrock\n    avec la trajectoire suivie par l'algorithme d'optimisation.\n    \"\"\"\n    # Créer une grille de points pour tracer les courbes de niveau\n    x = np.linspace(-2, 2, 400)\n    y = np.linspace(-1, 3, 400)\n    X, Y = np.meshgrid(x, y)\n    Z = rosenbrock(X, Y)\n    \n    # Créer la figure\n    fig, ax = plt.subplots(figsize=(12, 10))\n    \n    # Tracer les courbes de niveau avec une échelle logarithmique\n    levels = np.logspace(-1, 3.5, 35)\n    contour = ax.contour(X, Y, Z, levels=levels, cmap='viridis', alpha=0.6)\n    ax.clabel(contour, inline=True, fontsize=8, fmt='%.1f')\n    \n    # Extraire les coordonnées de la trajectoire\n    traj_x = [point['x'] for point in trajectory]\n    traj_y = [point['y'] for point in trajectory]\n    \n    # Tracer la trajectoire avec des points et des lignes\n    ax.plot(traj_x, traj_y, 'r.-', linewidth=2, markersize=8, \n            label='Trajectoire', alpha=0.7)\n    \n    # Marquer le point de départ en vert\n    ax.plot(traj_x[0], traj_y[0], 'go', markersize=15, \n            label=f'Départ ({traj_x[0]:.2f}, {traj_y[0]:.2f})', zorder=5)\n    \n    # Marquer le point d'arrivée en bleu\n    ax.plot(traj_x[-1], traj_y[-1], 'bo', markersize=15,\n            label=f'Arrivée ({traj_x[-1]:.4f}, {traj_y[-1]:.4f})', zorder=5)\n    \n    # Marquer le minimum global en rouge (étoile)\n    ax.plot(1, 1, 'r*', markersize=20, \n            label='Minimum global (1, 1)', zorder=5)\n    \n    # Ajouter quelques numéros d'itération le long de la trajectoire\n    step = max(1, len(trajectory) // 10)\n    for i in range(0, len(trajectory), step):\n        ax.annotate(f\"{trajectory[i]['iter']}\", \n                   (traj_x[i], traj_y[i]),\n                   textcoords=\"offset points\", xytext=(5,5), \n                   fontsize=8, color='darkred')\n    \n    ax.set_xlabel('x', fontsize=12)\n    ax.set_ylabel('y', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.legend(loc='best')\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_convergence(trajectory, title=\"Convergence de l'algorithme\"):\n    \"\"\"\n    Trace l'évolution de la valeur de la fonction objectif\n    et de la norme du gradient au cours des itérations.\n    \"\"\"\n    iterations = [point['iter'] for point in trajectory]\n    f_values = [point['f'] for point in trajectory]\n    grad_norms = [point['grad_norm'] for point in trajectory]\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Graphique 1 : Évolution de f(x, y)\n    ax1.semilogy(iterations, f_values, 'b-', linewidth=2)\n    ax1.set_xlabel('Itération k', fontsize=12)\n    ax1.set_ylabel('f(xₖ, yₖ)', fontsize=12)\n    ax1.set_title('Évolution de la fonction objectif', fontsize=13, fontweight='bold')\n    ax1.grid(True, alpha=0.3)\n    ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Minimum global')\n    ax1.legend()\n    \n    # Graphique 2 : Évolution de la norme du gradient\n    ax2.semilogy(iterations, grad_norms, 'g-', linewidth=2)\n    ax2.set_xlabel('Itération k', fontsize=12)\n    ax2.set_ylabel('||∇f(xₖ, yₖ)||', fontsize=12)\n    ax2.set_title('Évolution de la norme du gradient', fontsize=13, fontweight='bold')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n\ndef print_iteration_table(trajectory, n_rows=15):\n    \"\"\"\n    Affiche un tableau détaillé des premières et dernières itérations.\n    \"\"\"\n    print(\"=\" * 100)\n    print(\"TABLEAU DES ITÉRATIONS\")\n    print(\"=\" * 100)\n    \n    # En-tête du tableau\n    header = f\"{'k':>5} | {'xₖ':>12} | {'yₖ':>12} | {'f(xₖ,yₖ)':>15} | {'||∇f||':>15}\"\n    \n    # Vérifier si on a le pas optimal dans les données\n    if 's_k' in trajectory[0]:\n        header += f\" | {'sₖ':>12}\"\n    \n    print(header)\n    print(\"-\" * 100)\n    \n    # Afficher les n_rows premières itérations\n    n_display = min(n_rows, len(trajectory))\n    for point in trajectory[:n_display]:\n        row = f\"{point['iter']:>5} | {point['x']:>12.6f} | {point['y']:>12.6f} | {point['f']:>15.6e} | {point['grad_norm']:>15.6e}\"\n        if 's_k' in point and point['s_k'] is not None:\n            row += f\" | {point['s_k']:>12.6f}\"\n        print(row)\n    \n    # Si la trajectoire est longue, afficher ...\n    if len(trajectory) > 2 * n_rows:\n        print(\"  ...  |     ...      |     ...      |      ...        |      ...        \")\n        \n        # Afficher les n_rows dernières itérations\n        for point in trajectory[-n_rows:]:\n            row = f\"{point['iter']:>5} | {point['x']:>12.6f} | {point['y']:>12.6f} | {point['f']:>15.6e} | {point['grad_norm']:>15.6e}\"\n            if 's_k' in point and point['s_k'] is not None:\n                row += f\" | {point['s_k']:>12.6f}\"\n            print(row)\n    \n    print(\"=\" * 100)\n    \n    # Afficher un résumé final\n    final_point = trajectory[-1]\n    print(f\"\\nRÉSUMÉ:\")\n    print(f\"  Nombre total d'itérations : {final_point['iter']}\")\n    print(f\"  Point final : ({final_point['x']:.8f}, {final_point['y']:.8f})\")\n    print(f\"  Valeur finale : f = {final_point['f']:.6e}\")\n    print(f\"  Norme du gradient final : ||∇f|| = {final_point['grad_norm']:.6e}\")\n    print(f\"  Erreur au minimum : |x-1| = {abs(final_point['x']-1):.6e}, |y-1| = {abs(final_point['y']-1):.6e}\")\n    print(\"=\" * 100)\n\nprint(\"Fonctions de visualisation définies !\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:09:37.324171Z","iopub.execute_input":"2025-11-23T16:09:37.325277Z","iopub.status.idle":"2025-11-23T16:09:37.357655Z","shell.execute_reply.started":"2025-11-23T16:09:37.325197Z","shell.execute_reply":"2025-11-23T16:09:37.356521Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **6. Expérimentation : Gradient à Pas Optimal**\n\n#### Nous allons maintenant exécuter la méthode de plus profonde descente à partir du point initial $(-1, 2)$.\n","metadata":{"editable":false}},{"cell_type":"code","source":"#Point de départ\nx0, y0 = -1, 2\n\nprint(\"=\"*90)\nprint(\"EXPÉRIMENTATION 1 : MÉTHODE DE GRADIENT À PAS OPTIMAL\")\nprint(\"=\"*90)\nprint(f\"\\nPoint de départ : ({x0}, {y0})\")\n# Vérifier que les fonctions sont définies\ntry:\n    rosenbrock(0, 0)\n    print(\"✓ Fonctions correctement définies\")\nexcept NameError:\n    print(\"✗ ERREUR : Exécutez d'abord les cellules de définition des fonctions !\")\nprint(f\"Valeur initiale : f({x0}, {y0}) = {rosenbrock(x0, y0):.6f}\")\nprint(\"\\nExécution de l'algorithme...\\n\")\n\n# Exécuter l'algorithme\ntraj_optimal = gradient_pas_optimal(x0, y0, epsilon=1e-5, max_iter=10000)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **6.1 Tableau des itérations**","metadata":{}},{"cell_type":"code","source":"print_iteration_table(traj_optimal, n_rows=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **6.2 Visualisation de la trajectoire**","metadata":{}},{"cell_type":"code","source":"plot_contours_with_trajectory(traj_optimal, \n                              \"Rosenbrock : Gradient à pas optimal (Steepest Descent)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **6.3 Courbes de convergence**","metadata":{}},{"cell_type":"code","source":"plot_convergence(traj_optimal, \"Rosenbrock : Convergence avec pas optimal\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **6.4 Observations sur le phénomène de zigzag**\n\n#### Comme observé dans le cours (pages 32-34), la méthode de plus profonde descente présente un comportement en zigzag. \n \n#### Vérifions l'orthogonalité des gradients successifs :\n","metadata":{}},{"cell_type":"code","source":"print(\"Vérification de l'orthogonalité des gradients successifs :\")\nprint(\"=\"*70)\nprint(f\"{'Itération k':>12} | {'⟨∇f(xₖ), ∇f(xₖ₊₁)⟩':>25} | {'Orthogonaux?':>15}\")\nprint(\"-\"*70)\n\n# Prendre les 10 premières itérations\nfor i in range(min(10, len(traj_optimal)-1)):\n    point_k = traj_optimal[i]\n    point_k1 = traj_optimal[i+1]\n    \n    grad_k = np.array(gradient_rosenbrock(point_k['x'], point_k['y']))\n    grad_k1 = np.array(gradient_rosenbrock(point_k1['x'], point_k1['y']))\n    \n    produit_scalaire = np.dot(grad_k, grad_k1)\n    orthogonal = \"Oui\" if abs(produit_scalaire) < 1e-6 else \"Non\"\n    \n    print(f\"{i:>12} | {produit_scalaire:>25.6e} | {orthogonal:>15}\")\n\nprint(\"=\"*70)\nprint(\"\\nConclusion : Les gradients successifs sont (quasiment) orthogonaux,\")\nprint(\"ce qui explique le comportement en zigzag de la trajectoire.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **7. Expérimentation : Gradient à Pas Fixe**\n\n#### Nous allons maintenant tester la méthode de gradient à pas fixe avec différentes valeurs du pas.","metadata":{}},{"cell_type":"code","source":"def compare_fixed_steps(x0, y0, step_sizes, epsilon=1e-5, max_iter=100000):\n    \"\"\"\n    Compare les performances de l'algorithme de gradient à pas fixe\n    pour différentes valeurs du pas.\n    \"\"\"\n    results = []\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPARAISON DES DIFFÉRENTS PAS FIXES\")\n    print(\"=\"*80)\n    print(f\"{'Pas s':>12} | {'Converge?':>10} | {'Nb itérations':>15} | {'Valeur finale':>18} | {'||∇f|| final':>18}\")\n    print(\"-\"*80)\n    \n    for s in step_sizes:\n        trajectory, converged = gradient_pas_fixe(x0, y0, s, epsilon, max_iter)\n        \n        if converged:\n            final = trajectory[-1]\n            results.append({\n                'step': s,\n                'converged': True,\n                'iterations': final['iter'],\n                'final_value': final['f'],\n                'final_grad_norm': final['grad_norm'],\n                'trajectory': trajectory\n            })\n            print(f\"{s:>12.6f} | {'Oui':>10} | {final['iter']:>15} | {final['f']:>18.6e} | {final['grad_norm']:>18.6e}\")\n        else:\n            results.append({\n                'step': s,\n                'converged': False,\n                'iterations': len(trajectory),\n                'trajectory': trajectory\n            })\n            print(f\"{s:>12.6f} | {'Non (DV)':>10} | {len(trajectory):>15} | {'Divergence':>18} | {'-':>18}\")\n    \n    print(\"=\"*80)\n    \n    return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **7.1 Test de différents pas**\n","metadata":{}},{"cell_type":"code","source":"print(\"=\"*90)\nprint(\"EXPÉRIMENTATION 2 : MÉTHODE DE GRADIENT À PAS FIXE\")\nprint(\"=\"*90)\n\n# Tester une gamme de pas\npas_a_tester = [0.0001, 0.0003, 0.0005, 0.001, 0.002, 0.003, 0.005, 0.01]\n\nresultats_pas_fixes = compare_fixed_steps(x0, y0, pas_a_tester, \n                                          epsilon=1e-5, max_iter=100000)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **7.2 Visualisation du meilleur cas**","metadata":{}},{"cell_type":"code","source":"# Trouver le meilleur cas (celui qui converge le plus rapidement)\ncas_convergents = [r for r in resultats_pas_fixes if r['converged']]\n\nif cas_convergents:\n    meilleur_cas = min(cas_convergents, key=lambda x: x['iterations'])\n    print(f\"\\n{'='*90}\")\n    print(f\"MEILLEUR PAS FIXE : s = {meilleur_cas['step']}\")\n    print(f\"{'='*90}\")\n    print(f\"Nombre d'itérations : {meilleur_cas['iterations']}\")\n    print(f\"Ratio par rapport au pas optimal : {meilleur_cas['iterations'] / traj_optimal[-1]['iter']:.2f}\")\n    \n    # Tableau\n    print_iteration_table(meilleur_cas['trajectory'], n_rows=10)\n    \n    # Visualisation\n    plot_contours_with_trajectory(meilleur_cas['trajectory'],\n                                  f\"Rosenbrock : Gradient à pas fixe (s = {meilleur_cas['step']})\")\n    \n    plot_convergence(meilleur_cas['trajectory'],\n                    f\"Rosenbrock : Convergence avec s = {meilleur_cas['step']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **7.3 Visualisation d'un cas plus lent**","metadata":{}},{"cell_type":"code","source":"if len(cas_convergents) > 1:\n    # Prendre un cas avec convergence plus lente (pas plus petit)\n    cas_lents = [r for r in cas_convergents if r['step'] < meilleur_cas['step']]\n    if cas_lents:\n        cas_lent = min(cas_lents, key=lambda x: x['step'])\n        print(f\"\\n{'='*90}\")\n        print(f\"CAS AVEC PAS PLUS PETIT : s = {cas_lent['step']}\")\n        print(f\"{'='*90}\")\n        print(f\"Nombre d'itérations : {cas_lent['iterations']}\")\n        print(f\"Ratio par rapport au pas optimal : {cas_lent['iterations'] / traj_optimal[-1]['iter']:.2f}\")\n        \n        plot_contours_with_trajectory(cas_lent['trajectory'],\n                                      f\"Rosenbrock : Gradient à pas fixe (s = {cas_lent['step']})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **7.4 Visualisation d'un cas de divergence (si applicable)**\n","metadata":{}},{"cell_type":"code","source":"cas_divergents = [r for r in resultats_pas_fixes if not r['converged']]\n\nif cas_divergents:\n    cas_div = cas_divergents[0]\n    print(f\"\\n{'='*90}\")\n    print(f\"CAS DE DIVERGENCE : s = {cas_div['step']}\")\n    print(f\"{'='*90}\")\n    print(f\"Divergence après {cas_div['iterations']} itérations\")\n    \n    if len(cas_div['trajectory']) > 1:\n        plot_contours_with_trajectory(cas_div['trajectory'],\n                                      f\"Rosenbrock : DIVERGENCE avec s = {cas_div['step']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **8. Comparaison Graphique : Pas Optimal vs Meilleurs Pas Fixes**","metadata":{}},{"cell_type":"code","source":"if cas_convergents:\n    fig, ax = plt.subplots(figsize=(14, 10))\n    \n    # Courbes de niveau\n    x = np.linspace(-2, 2, 400)\n    y = np.linspace(-1, 3, 400)\n    X, Y = np.meshgrid(x, y)\n    Z = rosenbrock(X, Y)\n    levels = np.logspace(-1, 3.5, 35)\n    contour = ax.contour(X, Y, Z, levels=levels, cmap='gray', alpha=0.3)\n    \n    # Trajectoire pas optimal\n    traj_x_opt = [p['x'] for p in traj_optimal]\n    traj_y_opt = [p['y'] for p in traj_optimal]\n    ax.plot(traj_x_opt, traj_y_opt, 'b.-', linewidth=2, markersize=6, \n            label=f'Pas optimal ({traj_optimal[-1][\"iter\"]} iter)', alpha=0.7)\n    \n    # Trajectoire meilleur pas fixe\n    traj_x_best = [p['x'] for p in meilleur_cas['trajectory']]\n    traj_y_best = [p['y'] for p in meilleur_cas['trajectory']]\n    ax.plot(traj_x_best, traj_y_best, 'r.-', linewidth=2, markersize=6,\n            label=f'Pas fixe s={meilleur_cas[\"step\"]} ({meilleur_cas[\"iterations\"]} iter)', alpha=0.7)\n    \n    # Marques\n    ax.plot(x0, y0, 'go', markersize=15, label='Départ', zorder=5)\n    ax.plot(1, 1, 'r*', markersize=20, label='Minimum (1,1)', zorder=5)\n    \n    ax.set_xlabel('x', fontsize=12)\n    ax.set_ylabel('y', fontsize=12)\n    ax.set_title('Comparaison : Pas Optimal vs Pas Fixe', fontsize=14, fontweight='bold')\n    ax.legend(loc='best', fontsize=11)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **9. Visualisation 3D (Bonus)**","metadata":{}},{"cell_type":"code","source":"def plot_3d_surface(trajectory=None):\n    \"\"\"\n    Crée une visualisation 3D de la fonction de Rosenbrock.\n    Si une trajectoire est fournie, elle est projetée sur la surface.\n    \"\"\"\n    # Créer la grille\n    x = np.linspace(-2, 2, 100)\n    y = np.linspace(-1, 3, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = rosenbrock(X, Y)\n    \n    # Limiter Z pour une meilleure visualisation\n    Z = np.minimum(Z, 500)\n    \n    # Créer la figure 3D\n    fig = plt.figure(figsize=(14, 10))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # Tracer la surface\n    surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7, \n                           edgecolor='none', antialiased=True)\n    \n    # Si une trajectoire est fournie, la tracer sur la surface\n    if trajectory is not None:\n        traj_x = np.array([point['x'] for point in trajectory])\n        traj_y = np.array([point['y'] for point in trajectory])\n        traj_z = np.array([point['f'] for point in trajectory])\n        \n        # Limiter les valeurs z pour la visualisation\n        traj_z = np.minimum(traj_z, 500)\n        \n        ax.plot(traj_x, traj_y, traj_z, 'r-', linewidth=3, label='Trajectoire')\n        ax.scatter(traj_x[0], traj_y[0], traj_z[0], c='green', s=100, \n                  marker='o', label='Départ', zorder=5)\n        ax.scatter(traj_x[-1], traj_y[-1], traj_z[-1], c='blue', s=100, \n                  marker='o', label='Arrivée', zorder=5)\n    \n    # Marquer le minimum\n    ax.scatter(1, 1, 0, c='red', s=200, marker='*', \n              label='Minimum global', zorder=5)\n    \n    ax.set_xlabel('x', fontsize=12)\n    ax.set_ylabel('y', fontsize=12)\n    ax.set_zlabel('f(x, y)', fontsize=12)\n    ax.set_title('Fonction de Rosenbrock en 3D', fontsize=14, fontweight='bold')\n    ax.legend()\n    \n    # Ajouter une barre de couleur\n    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualisation 3D avec la trajectoire du pas optimal\nplot_3d_surface(traj_optimal)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **10. Analyse Détaillée des Résultats**","metadata":{}},{"cell_type":"code","source":"def analyser_resultats_rosenbrock(resultats_pas_optimal, resultats_pas_fixes, x0, y0):\n    \"\"\"\n    Génère une analyse complète et structurée des résultats d'optimisation\n    pour la fonction de Rosenbrock.\n    \"\"\"\n    \n    print(\"\\n\" + \"=\"*90)\n    print(\" \"*25 + \"ANALYSE DES RÉSULTATS\")\n    print(\" \"*20 + \"Fonction de Rosenbrock\")\n    print(\"=\"*90)\n    \n    # ========== PARTIE 1 : PRÉSENTATION DE LA FONCTION ==========\n    print(\"\\n\" + \"-\"*90)\n    print(\"1. CARACTÉRISTIQUES DE LA FONCTION DE ROSENBROCK\")\n    print(\"-\"*90)\n    \n    print(\"\"\"\nLa fonction de Rosenbrock, également appelée fonction \"banane\" ou \"vallée de Rosenbrock\",\nest définie par :\n\n    f(x, y) = (1 - x)² + 100(y - x²)²\n\nCette fonction est un cas test classique en optimisation numérique car elle présente\nplusieurs propriétés intéressantes et difficiles à gérer pour les algorithmes d'optimisation.\n\nPropriétés principales :\n    • Minimum global unique en (x*, y*) = (1, 1) avec f(1, 1) = 0\n    • Fonction non-convexe (contrairement aux exemples du cours)\n    • Vallée étroite et incurvée en forme de banane menant au minimum\n    • Gradient faible le long de la vallée mais très raide perpendiculairement\n    • La convergence est difficile car il faut d'abord trouver la vallée, \n      puis la suivre sur une longue distance jusqu'au minimum\n\nCes caractéristiques rendent la fonction de Rosenbrock particulièrement intéressante\npour tester la robustesse et l'efficacité des algorithmes d'optimisation.\n\"\"\")\n    \n    print(f\"Point de départ utilisé : ({x0}, {y0})\")\n    print(f\"Valeur initiale de la fonction : f({x0}, {y0}) = {rosenbrock(x0, y0):.6e}\")\n    \n    # ========== PARTIE 2 : ANALYSE DE LA MÉTHODE À PAS OPTIMAL ==========\n    print(\"\\n\" + \"-\"*90)\n    print(\"2. ANALYSE DE LA MÉTHODE DE PLUS PROFONDE DESCENTE (PAS OPTIMAL)\")\n    print(\"-\"*90)\n    \n    n_iter_optimal = resultats_pas_optimal[-1]['iter']\n    valeur_finale_optimal = resultats_pas_optimal[-1]['f']\n    point_final_optimal = (resultats_pas_optimal[-1]['x'], resultats_pas_optimal[-1]['y'])\n    \n    print(f\"\"\"\nCette méthode calcule à chaque itération le pas optimal sₖ qui minimise\nf(xₖ - s∇f(xₖ)) par rapport à s. C'est la méthode idéalisée du gradient.\n\nRésultats obtenus :\n    • Nombre d'itérations : {n_iter_optimal}\n    • Point final : ({point_final_optimal[0]:.8f}, {point_final_optimal[1]:.8f})\n    • Valeur finale : f = {valeur_finale_optimal:.6e}\n    • Erreur au minimum : |x-1| = {abs(point_final_optimal[0]-1):.6e}\n                          |y-1| = {abs(point_final_optimal[1]-1):.6e}\n\"\"\")\n    \n    # Analyser le phénomène de zigzag\n    print(\"Observation du phénomène de zigzag :\")\n    print(\"\"\"\nComme observé dans le cours (pages 32-34) pour la fonction quadratique, la méthode\nde plus profonde descente présente un comportement caractéristique en zigzag.\nCe phénomène s'explique par une propriété mathématique fondamentale : deux directions\nde descente successives sont orthogonales.\n\nEn effet, si sₖ est le pas optimal à l'itération k, alors il vérifie la condition\nd'optimalité du premier ordre :\n\n    d/ds[f(xₖ - s∇f(xₖ))]|ₛ=ₛₖ = 0\n\nCe qui implique que : ⟨∇f(xₖ), ∇f(xₖ₊₁)⟩ = 0\n\nCette orthogonalité des gradients successifs se traduit visuellement par un mouvement\nen zigzag où l'algorithme effectue des allers-retours perpendiculaires dans la vallée,\nplutôt que de la suivre directement. Ce comportement est particulièrement prononcé\npour la fonction de Rosenbrock en raison de sa vallée étroite et courbée.\n\"\"\")\n    \n    # Analyser quelques pas optimaux\n    if len(resultats_pas_optimal) > 10:\n        pas_optimaux = [p['s_k'] for p in resultats_pas_optimal[1:11] if p['s_k'] is not None]\n        if pas_optimaux:\n            print(f\"Variation des pas optimaux (10 premières itérations) :\")\n            print(f\"    • Minimum : {min(pas_optimaux):.6f}\")\n            print(f\"    • Maximum : {max(pas_optimaux):.6f}\")\n            print(f\"    • Moyenne : {np.mean(pas_optimaux):.6f}\")\n            print(\"\"\"\nLa variation importante des pas optimaux montre que l'algorithme s'adapte à la\ngéométrie locale de la fonction. Dans les zones raides, le pas est petit, tandis\nque dans les zones plus plates, le pas peut être plus grand.\n\"\"\")\n    \n    # ========== PARTIE 3 : ANALYSE DES MÉTHODES À PAS FIXE ==========\n    print(\"\\n\" + \"-\"*90)\n    print(\"3. ANALYSE DES MÉTHODES À PAS FIXE\")\n    print(\"-\"*90)\n    \n    print(\"\"\"\nLa méthode à pas fixe est plus simple à implémenter car elle ne nécessite pas\nde résoudre un problème d'optimisation unidimensionnel à chaque itération.\nCependant, le choix du pas est critique pour garantir la convergence.\n\"\"\")\n    \n    # Séparer les résultats convergés et divergés\n    converges = [r for r in resultats_pas_fixes if r['converged']]\n    diverges = [r for r in resultats_pas_fixes if not r['converged']]\n    \n    if converges:\n        print(f\"\\nCas de convergence ({len(converges)} pas testés) :\")\n        print(\"-\" * 80)\n        \n        meilleur = min(converges, key=lambda x: x['iterations'])\n        \n        for res in sorted(converges, key=lambda x: x['step']):\n            ratio = res['iterations'] / n_iter_optimal if n_iter_optimal > 0 else float('inf')\n            print(f\"    • Pas s = {res['step']:.6f} :\")\n            print(f\"        - {res['iterations']} itérations (×{ratio:.2f} par rapport au pas optimal)\")\n            print(f\"        - Valeur finale : {res['final_value']:.6e}\")\n            print(f\"        - Point final : ({res['trajectory'][-1]['x']:.6f}, {res['trajectory'][-1]['y']:.6f})\")\n        \n        print(f\"\"\"\nObservation : Le meilleur pas fixe testé est s = {meilleur['step']:.6f} avec \n{meilleur['iterations']} itérations.\n\"\"\")\n        \n        print(\"\"\"\nAnalyse de l'influence du pas :\n\nPour la fonction de Rosenbrock, nous observons que :\n\n    1. Les pas très petits (s << 0.001) convergent mais très lentement.\n       L'algorithme progresse à tout petits pas et nécessite un très grand nombre\n       d'itérations. C'est une approche sûre mais inefficace.\n    \n    2. Il existe une plage de pas \"optimaux\" où la convergence est relativement\n       rapide. Ces pas permettent de faire des progrès significatifs à chaque\n       itération sans risquer de divergence.\n    \n    3. Les pas trop grands (s > seuil_critique) font diverger l'algorithme.\n       Les itérations \"sautent\" par-dessus le minimum et s'éloignent de plus\n       en plus de la solution.\n\nLa théorie de la convergence nous enseigne que pour garantir la convergence\nd'une méthode de gradient à pas fixe, il faut que le pas vérifie :\n\n    0 < s < 2/L\n\noù L est la constante de Lipschitz du gradient (liée à la plus grande valeur\npropre de la Hessienne). Pour Rosenbrock, cette constante varie avec la position,\nce qui explique pourquoi certains pas fonctionnent et d'autres non.\n\"\"\")\n    \n    if diverges:\n        print(f\"\\nCas de divergence ({len(diverges)} pas testés) :\")\n        print(\"-\" * 80)\n        \n        for res in sorted(diverges, key=lambda x: x['step']):\n            print(f\"    • Pas s = {res['step']:.6f} : Divergence après {res['iterations']} itérations\")\n        \n        print(\"\"\"\nCes divergences illustrent l'importance cruciale du choix du pas dans les méthodes\nà pas fixe. Un pas trop grand viole les conditions de convergence théoriques et\nl'algorithme s'éloigne du minimum au lieu de s'en rapprocher.\n\"\"\")\n    \n    # ========== PARTIE 4 : COMPARAISON GLOBALE ==========\n    print(\"\\n\" + \"-\"*90)\n    print(\"4. COMPARAISON GLOBALE ET RECOMMANDATIONS\")\n    print(\"-\"*90)\n    \n    print(\"\"\"\nSynthèse des observations :\n\"\"\")\n    \n    print(f\"\"\"\nMéthode à pas optimal :\n    • Avantages : \n        - Convergence garantie (sous certaines hypothèses)\n        - Pas besoin de régler de paramètre\n        - Progression \"optimale\" à chaque itération\n    • Inconvénients :\n        - Comportement en zigzag inefficace dans les vallées étroites\n        - Coût élevé : nécessite de résoudre un problème d'optimisation 1D \n          à chaque itération\n        - {n_iter_optimal} itérations pour cette fonction\n\"\"\")\n    \n    if converges:\n        print(f\"\"\"\nMéthode à pas fixe (meilleur cas : s = {meilleur['step']:.6f}) :\n    • Avantages :\n        - Simplicité d'implémentation\n        - Coût par itération minimal\n        - Peut être plus rapide que le pas optimal si s est bien choisi\n    • Inconvénients :\n        - Nécessite de choisir le bon pas (essais-erreurs ou théorie)\n        - Risque de divergence si s est trop grand\n        - Convergence lente si s est trop petit\n        - {meilleur['iterations']} itérations pour le meilleur pas testé\n\"\"\")\n    \n    print(\"\"\"\nRecommandations pratiques :\n\nPour la fonction de Rosenbrock et les fonctions similaires (non-convexes avec\nvallées étroites), les méthodes de gradient simple (pas optimal ou pas fixe)\nne sont pas les plus efficaces. Les observations suivantes sont importantes :\n\n    1. Le phénomène de zigzag ralentit considérablement la convergence dans\n       les vallées étroites. Des méthodes plus sophistiquées comme Newton ou\n       quasi-Newton (BFGS) sont préférables car elles tiennent compte de la\n       courbure (Hessienne) et peuvent \"suivre\" la vallée plus efficacement.\n    \n    2. Pour le gradient à pas fixe, le choix du pas est délicat et dépend\n       de la région de l'espace où l'on se trouve. Une stratégie de pas\n       adaptatif serait plus robuste.\n    \n    3. La convergence théorique (||∇f|| → 0) ne garantit pas qu'on atteigne\n       le minimum global pour des fonctions non-convexes. Le point initial\n       est donc crucial.\n\nCes limitations des méthodes de gradient justifient l'étude des méthodes\nplus avancées vues dans le cours (Newton, quasi-Newton, gradient conjugué).\n\"\"\")\n    \n    print(\"=\"*90 + \"\\n\")\n    \n    return {\n        'n_iter_optimal': n_iter_optimal,\n        'meilleur_pas_fixe': meilleur if converges else None,\n        'pas_divergents': [r['step'] for r in diverges]\n    }\n\n# Exécuter l'analyse complète\nanalyse = analyser_resultats_rosenbrock(traj_optimal, resultats_pas_fixes, x0, y0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **11. Graphique de Synthèse : Nombre d'itérations vs Taille du pas**","metadata":{}},{"cell_type":"code","source":"if cas_convergents:\n    # Préparer les données\n    pas_values = [r['step'] for r in cas_convergents]\n    iter_values = [r['iterations'] for r in cas_convergents]\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Tracer le graphique\n    ax.plot(pas_values, iter_values, 'bo-', linewidth=2, markersize=8, label='Pas fixe')\n    \n    # Ligne horizontale pour le pas optimal\n    ax.axhline(y=traj_optimal[-1]['iter'], color='r', linestyle='--', \n               linewidth=2, label=f'Pas optimal ({traj_optimal[-1][\"iter\"]} iter)')\n    \n    ax.set_xlabel('Taille du pas (s)', fontsize=12)\n    ax.set_ylabel('Nombre d\\'itérations', fontsize=12)\n    ax.set_title('Influence de la taille du pas sur la convergence', \n                 fontsize=14, fontweight='bold')\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=11)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nObservation :\")\n    print(\"-\" * 80)\n    print(\"Le graphique montre clairement la relation entre la taille du pas et\")\n    print(\"le nombre d'itérations nécessaires pour converger :\")\n    print(\"  • Pas très petit → beaucoup d'itérations (convergence lente)\")\n    print(\"  • Pas optimal intermédiaire → nombre d'itérations réduit\")\n    print(\"  • Pas trop grand → divergence (non représenté)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **12. Conclusion**\n\n#### **Résumé des résultats**\n\n#### Dans ce notebook, nous avons étudié l'optimisation de la fonction de Rosenbrock avec deux méthodes de gradient :\n\n#### 1. **Gradient à pas optimal** : Convergence en {n_iter_optimal} itérations mais avec un comportement en zigzag caractéristique\n\n#### 2. **Gradient à pas fixe** : Les performances varient fortement selon la taille du pas :\n####    - Pas trop petit : convergence très lente\n####    - Pas bien choisi : performances comparables au pas optimal\n####    - Pas trop grand : divergence\n###\n#### **Observations principales**\n\n#### - Le phénomène de **zigzag** observé avec le pas optimal est dû à l'orthogonalité des gradients successifs\n#### - La **vallée étroite** de Rosenbrock rend la convergence difficile pour les méthodes de gradient simple\n#### - Le choix du **pas fixe** est crucial et dépend de la géométrie locale de la fonction\n#### - Ces méthodes simples sont **inefficaces** pour ce type de fonction, justifiant l'usage de méthodes plus avancées\n###\n#### **Perspectives**\n\n#### Pour améliorer les performances sur la fonction de Rosenbrock, il faudrait utiliser :\n#### - La méthode de **Newton** qui utilise la Hessienne\n#### - Les méthodes **quasi-Newton** (BFGS, L-BFGS)\n#### - Le **gradient conjugué**\n#### - Des stratégies de **pas adaptatif** (Armijo, Wolfe)\n####\n#### Ces méthodes sont étudiées dans la suite du cours et permettent de mieux gérer les vallées étroites.\n\n#\n# ---\n### **Fin du notebook pour la fonction de Rosenbrock**\n\n#### Dans les prochains notebooks, nous appliquerons la même méthodologie aux fonctions :\n#### - Quadratique : $f(x, y) = x^2 - y^2$\n#### - Himmelblau : $f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}